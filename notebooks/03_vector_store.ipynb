{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Notebook\n",
    "\n",
    "This notebook handles vector store operations for the RAG system.\n",
    "\n",
    "## Purpose\n",
    "This notebook demonstrates how to manage the vector database, which is crucial for efficient retrieval. It covers:\n",
    "\n",
    "1.  **Initialization**: Setting up the vector store using FAISS or ChromaDB.\n",
    "2.  **Indexing**: Adding embeddings along with their metadata to the index.\n",
    "3.  **Search**: Performing semantic similarity searches to find relevant documents for a given query.\n",
    "4.  **Persistence**: Saving and loading the vector index to/from disk.\n",
    "\n",
    "## Usage\n",
    "Import vector store functions from `src.rag.vector_store` and use this notebook to manage your vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.cirq_rag_code_assistant.config import get_config, setup_logging\n",
    "from src.rag.vector_store import VectorStore\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vector Store\n",
    "We will initialize the VectorStore with a specific embedding dimension. We'll use FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_DIM = 384  # Example dimension (e.g., for all-MiniLM-L6-v2)\n",
    "VECTOR_DB_TYPE = \"faiss\"\n",
    "INDEX_PATH = project_root / \"outputs\" / \"vector_store_test\"\n",
    "\n",
    "# Initialize\n",
    "vector_store = VectorStore(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    vector_db_type=VECTOR_DB_TYPE,\n",
    "    index_path=INDEX_PATH,\n",
    "    use_gpu=False  # Set to True if you have a GPU and faiss-gpu installed\n",
    ")\n",
    "\n",
    "print(f\"Vector Store initialized: {vector_store.vector_db_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Embeddings\n",
    "Let's add some dummy embeddings and metadata to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "num_items = 10\n",
    "embeddings = np.random.rand(num_items, EMBEDDING_DIM).astype('float32')\n",
    "\n",
    "# Normalize embeddings (simulating what an embedding model would do)\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = embeddings / norms\n",
    "\n",
    "ids = [f\"doc_{i}\" for i in range(num_items)]\n",
    "metadatas = [\n",
    "    {\"type\": \"code\", \"language\": \"python\", \"topic\": f\"topic_{i%3}\"}\n",
    "    for i in range(num_items)\n",
    "]\n",
    "\n",
    "# Add to store\n",
    "vector_store.add(embeddings, ids, metadatas)\n",
    "\n",
    "print(f\"Added {num_items} items. Total size: {vector_store.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "Now we can perform a similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a query vector\n",
    "query_embedding = np.random.rand(1, EMBEDDING_DIM).astype('float32')\n",
    "query_norm = np.linalg.norm(query_embedding)\n",
    "query_embedding = query_embedding / query_norm\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "results = vector_store.search(query_embedding, k=k)\n",
    "\n",
    "print(f\"Top {k} results:\")\n",
    "for res in results[0]:\n",
    "    print(f\"ID: {res['id']}, Score: {res['score']:.4f}, Metadata: {res['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "We can also filter results by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by topic\n",
    "filter_dict = {\"topic\": \"topic_0\"}\n",
    "results_filtered = vector_store.search(query_embedding, k=k, filter_dict=filter_dict)\n",
    "\n",
    "print(f\"Top {k} results with filter {filter_dict}:\")\n",
    "for res in results_filtered[0]:\n",
    "    print(f\"ID: {res['id']}, Score: {res['score']:.4f}, Metadata: {res['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load\n",
    "Finally, let's save the index to disk and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "vector_store.save()\n",
    "print(f\"Vector store saved to {INDEX_PATH}\")\n",
    "\n",
    "# Load new instance\n",
    "new_vector_store = VectorStore(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    vector_db_type=VECTOR_DB_TYPE,\n",
    "    index_path=INDEX_PATH\n",
    ")\n",
    "new_vector_store.load()\n",
    "\n",
    "print(f\"Loaded vector store. Size: {new_vector_store.size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}