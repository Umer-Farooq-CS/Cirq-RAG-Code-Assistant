{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing Notebook\n",
        "\n",
        "This notebook handles data preprocessing for the Cirq-RAG-Code-Assistant project.\n",
        "\n",
        "## Purpose\n",
        "- Fetch quantum code from GitHub repositories\n",
        "- Load and clean knowledge base data\n",
        "- Process Cirq code snippets\n",
        "- Generate descriptions for code samples\n",
        "- Prepare data for embedding generation\n",
        "- Organize knowledge base structure\n",
        "\n",
        "## Usage\n",
        "Import preprocessing functions from `src.data` and use them to process your data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Import the necessary modules for data fetching, preprocessing, and loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import data processing modules\n",
        "from pathlib import Path\n",
        "from src.data.fetcher import DatasetFetcher\n",
        "from src.data.preprocessor import DataPreprocessor\n",
        "from src.data.description_generator import DescriptionGenerator\n",
        "from src.data.dataset_loader import DatasetLoader\n",
        "\n",
        "# Set up paths\n",
        "DATA_DIR = Path(\"data/datasets\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch Data from GitHub\n",
        "\n",
        "Fetch Cirq code samples from the Cirq GitHub repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize fetcher\n",
        "fetcher = DatasetFetcher(\n",
        "    repos_dir=\"repos\",  # Directory to clone repositories\n",
        "    output_dir=DATA_DIR,  # Output directory for extracted data\n",
        ")\n",
        "\n",
        "# Fetch code from all repositories\n",
        "# Note: This will clone repositories if they don't exist\n",
        "# Set force_clone=True to re-clone existing repositories\n",
        "output_file = fetcher.fetch_all(\n",
        "    output_filename=\"quantum_code_samples_filtered.jsonl\",\n",
        "    force_clone=False,  # Set to True to re-clone\n",
        "    min_code_length=50,\n",
        "    max_code_length=50000,\n",
        ")\n",
        "\n",
        "print(f\"âœ… Data fetched and saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Inspect Dataset\n",
        "\n",
        "Load the dataset and view statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset_path = DATA_DIR / \"quantum_code_samples_filtered.jsonl\"\n",
        "loader = DatasetLoader(dataset_path)\n",
        "\n",
        "# Print statistics\n",
        "loader.print_stats()\n",
        "\n",
        "# Get some sample entries\n",
        "samples = loader.sample(3, seed=42)\n",
        "print(\"\\nðŸ“‹ Sample entries:\")\n",
        "for i, entry in enumerate(samples, 1):\n",
        "    print(f\"\\n--- Sample {i} ---\")\n",
        "    print(f\"Framework: {entry.get('framework')}\")\n",
        "    print(f\"File: {entry.get('file')}\")\n",
        "    print(f\"Code length: {len(entry.get('code', ''))} characters\")\n",
        "    print(f\"Code preview: {entry.get('code', '')[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess Dataset\n",
        "\n",
        "Clean and validate the dataset, remove duplicates, and extract metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = DataPreprocessor(\n",
        "    min_code_length=50,\n",
        "    max_code_length=50000,\n",
        "    min_lines=5,\n",
        "    max_lines=1000,\n",
        "    remove_duplicates=True,\n",
        "    validate_syntax=True,\n",
        ")\n",
        "\n",
        "# Preprocess dataset\n",
        "input_file = DATA_DIR / \"quantum_code_samples_filtered.jsonl\"\n",
        "output_file = DATA_DIR / \"quantum_code_samples_preprocessed.jsonl\"\n",
        "\n",
        "stats = preprocessor.preprocess_dataset(\n",
        "    input_path=input_file,\n",
        "    output_path=output_file,\n",
        "    add_metadata=True,\n",
        ")\n",
        "\n",
        "print(f\"âœ… Preprocessing complete! Processed {stats['processed']} entries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Descriptions\n",
        "\n",
        "Add natural language descriptions to code samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize description generator\n",
        "# Set use_ml=True to use ML-based summarization (requires transformers)\n",
        "generator = DescriptionGenerator(\n",
        "    use_ml=False,  # Set to True for ML-enhanced descriptions\n",
        "    ml_model=\"facebook/bart-large-cnn\",\n",
        "    device=\"auto\",  # \"auto\", \"cpu\", or \"cuda\"\n",
        ")\n",
        "\n",
        "# Generate descriptions\n",
        "input_file = DATA_DIR / \"quantum_code_samples_preprocessed.jsonl\"\n",
        "output_file = DATA_DIR / \"quantum_dataset_with_descriptions.jsonl\"\n",
        "\n",
        "desc_stats = generator.add_descriptions_to_dataset(\n",
        "    input_path=input_file,\n",
        "    output_path=output_file,\n",
        "    use_ml=False,  # Override instance setting if needed\n",
        "    batch_size=100,\n",
        ")\n",
        "\n",
        "print(f\"âœ… Descriptions generated! Processed {desc_stats['processed']} entries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify Final Dataset\n",
        "\n",
        "Load and verify the final preprocessed dataset with descriptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load final dataset\n",
        "final_dataset = DatasetLoader(DATA_DIR / \"quantum_dataset_with_descriptions.jsonl\")\n",
        "\n",
        "# Print statistics\n",
        "final_dataset.print_stats()\n",
        "\n",
        "# View a sample entry with description\n",
        "samples = final_dataset.sample(1, seed=42)\n",
        "if samples:\n",
        "    entry = samples[0]\n",
        "    print(\"\\nðŸ“‹ Sample entry with description:\")\n",
        "    print(f\"Framework: {entry.get('framework')}\")\n",
        "    print(f\"File: {entry.get('file')}\")\n",
        "    print(f\"\\nDescription:\")\n",
        "    print(entry.get('description', 'No description'))\n",
        "    print(f\"\\nMetadata:\")\n",
        "    if 'metadata' in entry:\n",
        "        for key, value in entry['metadata'].items():\n",
        "            print(f\"  - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. View Cirq Samples\n",
        "\n",
        "View and analyze Cirq samples from the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all Cirq samples (all entries should be Cirq)\n",
        "cirq_samples = final_dataset.get_by_framework(\"Cirq\")\n",
        "print(f\"Found {len(cirq_samples)} Cirq samples\")\n",
        "\n",
        "# View a Cirq sample\n",
        "if cirq_samples:\n",
        "    sample = cirq_samples[0]\n",
        "    print(f\"\\nðŸ“‹ Cirq Sample:\")\n",
        "    print(f\"File: {sample.get('file')}\")\n",
        "    print(f\"Description: {sample.get('description', 'No description')[:200]}...\")\n",
        "    print(f\"\\nCode preview:\")\n",
        "    print(sample.get('code', '')[:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Complete Pipeline\n",
        "\n",
        "Run the complete preprocessing pipeline in one go.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete preprocessing pipeline\n",
        "def run_preprocessing_pipeline(\n",
        "    fetch_data: bool = False,\n",
        "    generate_descriptions: bool = True,\n",
        "    use_ml: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the complete data preprocessing pipeline.\n",
        "    \n",
        "    Args:\n",
        "        fetch_data: Whether to fetch data from GitHub\n",
        "        generate_descriptions: Whether to generate descriptions\n",
        "        use_ml: Whether to use ML for description generation\n",
        "    \"\"\"\n",
        "    # Step 1: Fetch data (optional, if not already done)\n",
        "    if fetch_data:\n",
        "        print(\"Step 1: Fetching data from GitHub...\")\n",
        "        fetcher = DatasetFetcher(output_dir=DATA_DIR)\n",
        "        fetcher.fetch_all()\n",
        "    \n",
        "    # Step 2: Preprocess data\n",
        "    print(\"\\nStep 2: Preprocessing data...\")\n",
        "    preprocessor = DataPreprocessor()\n",
        "    preprocessor.preprocess_dataset(\n",
        "        input_path=DATA_DIR / \"quantum_code_samples_filtered.jsonl\",\n",
        "        output_path=DATA_DIR / \"quantum_code_samples_preprocessed.jsonl\",\n",
        "    )\n",
        "    \n",
        "    # Step 3: Generate descriptions\n",
        "    if generate_descriptions:\n",
        "        print(\"\\nStep 3: Generating descriptions...\")\n",
        "        generator = DescriptionGenerator(use_ml=use_ml)\n",
        "        generator.add_descriptions_to_dataset(\n",
        "            input_path=DATA_DIR / \"quantum_code_samples_preprocessed.jsonl\",\n",
        "            output_path=DATA_DIR / \"quantum_dataset_with_descriptions.jsonl\",\n",
        "        )\n",
        "    \n",
        "    print(\"\\nâœ… Pipeline complete!\")\n",
        "\n",
        "# Uncomment to run the complete pipeline:\n",
        "# run_preprocessing_pipeline(fetch_data=False, generate_descriptions=True, use_ml=False)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
