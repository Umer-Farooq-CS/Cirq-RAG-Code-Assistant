{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "This notebook implements the evaluation framework for the system.\n",
    "\n",
    "## Purpose\n",
    "This notebook is used to assess the performance and quality of the system. It covers:\n",
    "\n",
    "1.  **Metrics Calculation**: Computing specific metrics like circuit depth, gate count, and code validity.\n",
    "2.  **Benchmarking**: Running the system against a set of standard quantum algorithms (e.g., VQE, Grover) to measure success rates.\n",
    "3.  **Ablation Studies**: systematically disabling components (e.g., RAG, Optimizer) to evaluate their impact on performance.\n",
    "\n",
    "## Usage\n",
    "Run this notebook to perform rigorous evaluation and benchmarking of the Cirq-RAG-Code-Assistant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cirq\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.cirq_rag_code_assistant.config import get_config, setup_logging\n",
    "from src.evaluation.metrics import MetricsCollector\n",
    "from src.evaluation.ablation import AblationStudy\n",
    "from src.tools.analyzer import CircuitAnalyzer\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculation\n",
    "Let's calculate metrics for a sample circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample circuit\n",
    "q0, q1 = cirq.LineQubit.range(2)\n",
    "circuit = cirq.Circuit(\n",
    "    cirq.H(q0),\n",
    "    cirq.CNOT(q0, q1),\n",
    "    cirq.measure(q0, q1)\n",
    ")\n",
    "\n",
    "# Analyze\n",
    "analyzer = CircuitAnalyzer()\n",
    "analysis = analyzer.analyze(circuit)\n",
    "\n",
    "print(\"Circuit Metrics:\")\n",
    "for key, value in analysis['metrics'].items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Study\n",
    "Let's run a mini ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define benchmark cases\n",
    "cases = [\n",
    "    {\"query\": \"Create a Bell state\", \"algorithm\": \"teleportation\"},\n",
    "    {\"query\": \"Create a GHZ state\", \"algorithm\": \"ghz\"}\n",
    "]\n",
    "\n",
    "# Initialize Study\n",
    "study = AblationStudy(benchmark_cases=cases)\n",
    "\n",
    "# Run variants\n",
    "variants = [\"full\", \"no_optimizer\"]\n",
    "\n",
    "try:\n",
    "    results = study.run_study(variants)\n",
    "    \n",
    "    print(\"\\nAblation Results:\")\n",
    "    for variant, res in results.items():\n",
    "        print(f\"\\nVariant: {variant}\")\n",
    "        print(f\"Success Rate: {res['success_rate']:.2f}\")\n",
    "        print(f\"Avg Latency: {res['avg_latency']:.4f}s\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error running study: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}